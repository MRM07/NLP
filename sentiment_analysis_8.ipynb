{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "sentiment-analysis-8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRM07/NLP/blob/master/sentiment_analysis_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "69lURY_laRUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "    # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "    # For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD-g0OoyaRU5",
        "colab_type": "code",
        "outputId": "1cf626f2-f259-4e0b-a79e-122832bc6b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Ignore  the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "% matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "  # lammatizer from WordNet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# for part-of-speech tagging\n",
        "from nltk import pos_tag\n",
        "\n",
        "# for named entity recognition (NER)\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# BeautifulSoup libraray\n",
        "from bs4 import BeautifulSoup \n",
        "\n",
        "import re # regex\n",
        "\n",
        "#model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score \n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#preprocessing scikit\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n",
        "\n",
        "#classifiaction.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        " \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "#gensim w2v\n",
        "#word2vec\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-lVbMUGaRU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#keras\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM, SpatialDropout1D\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcftu-F4aRVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-5g9DgdaRVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df= pd.read_csv(\"train.csv\")\n",
        "test_df =pd.read_csv(\"test.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU3n66B1aRVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Regular expressions help us deal with characters and numerics and modify them according to our requirement \n",
        "import re\n",
        "#import the nltk library\n",
        "from nltk.corpus import stopwords\n",
        "sw = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vljo0BADaRVK",
        "colab_type": "code",
        "outputId": "1d570c24-f1ad-401a-a168-e75d6050a1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "train_df.groupby('sentiment').drug.count().plot.bar(ylim=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGTCAYAAADgNPLXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogfUfzBmaRVO",
        "colab_type": "text"
      },
      "source": [
        "The dataset is largely imbalanced in the sense that most of the drugs have neutral reviews. Thus our model will be biased towards predicting neutral sentiment unless we use some kind of sampling technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmLTEkhRaRVP",
        "colab_type": "text"
      },
      "source": [
        "Our dataset has 4 columns- \n",
        "* Hash\n",
        "* Drug Review\n",
        "* Drug Name\n",
        "* Sentiment\n",
        "\n",
        "We'll make our prediction using hash as the index and label(Sentiment ) in the submission file\n",
        "\n",
        "**Let's proceed to doing some preprocessing in the text, and identify the average length of sentences, phrases having the max occurences, no. of punctuations, exclamations, removal of stopwords, unnecessary spaces, and other noises**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVB1QHUqaRVQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gE58HbBaRVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to clean and pre-process the text.\n",
        "def clean_text(text):  \n",
        "    \n",
        "    # 1. Removing html tags\n",
        "#     review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
        "    \n",
        "    # 2. Retaining only alphabets.\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    \n",
        "    # 3. Converting to lower case and splitting\n",
        "    word_tokens= review_text.lower().split()\n",
        "    \n",
        "    # 4. Remove stopwords\n",
        "    le=WordNetLemmatizer()\n",
        "    stop_words= set(stopwords.words(\"english\"))     \n",
        "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "    cleaned_review=\" \".join(word_tokens)\n",
        "    return cleaned_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA993AJPaRVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df[\"clean_text\"]= train_df[\"text\"].apply(clean_text)\n",
        "test_df[\"clean_text\"]= test_df[\"text\"].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq-rYGgMaRVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text= train_df[\"clean_text\"]\n",
        "test_text= test_df[\"clean_text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwdCCF2KaRVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_text= pd.concat([train_text, test_text])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz0LMi5gaRVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsP0oJ1WaRVl",
        "colab_type": "code",
        "outputId": "e6cd0376-7235-45e3-f1a3-4a7ea9b5da62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "count_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_df=0.60,\n",
        "                        tokenizer=nltk.word_tokenize,\n",
        "                        strip_accents='unicode',\n",
        "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
        "                        use_idf=True\n",
        "                        )\n",
        "bag_of_words = count_vectorizer.fit_transform(train_text)\n",
        "print(bag_of_words.shape)\n",
        "X_test = count_vectorizer.transform(test_text)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5279, 33655)\n",
            "(2924, 33655)\n",
            "CPU times: user 9.99 s, sys: 30.9 ms, total: 10 s\n",
            "Wall time: 10 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9nBDypRaRVq",
        "colab_type": "code",
        "outputId": "9c832c80-a010-4f36-b7d5-8938ab06e1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bag_of_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5279x33451 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 519969 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7pLzQRlaRVx",
        "colab_type": "code",
        "outputId": "cc854010-e1b4-4e22-ce50-ed4ded1b9811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "transformer = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "transformer_bag_of_words = transformer.fit_transform(bag_of_words)\n",
        "X_test_transformer = transformer.transform(X_test)\n",
        "print (transformer_bag_of_words.shape)\n",
        "print (X_test_transformer.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5279, 33655)\n",
            "(2924, 33655)\n",
            "CPU times: user 71.5 ms, sys: 7.05 ms, total: 78.5 ms\n",
            "Wall time: 83.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAQrBdyENWkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_arr=[]\n",
        "train_array=transformer_bag_of_words.toarray()\n",
        "for x in len(train_array):\n",
        "  train_arr.append(x)\n",
        "  \n",
        "train_arr  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkdocdFxaRV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y= train_df[\"sentiment\"].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbrz4raWMWIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwFSHMU8aRV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare train and validation sets \n",
        "from sklearn.model_selection import train_test_split\n",
        "# Y = pd.get_dummies(train_df['sentiment']).values  # one hot target as required by NN.\n",
        "X_train,X_test,y_train,y_test=train_test_split(transformer_bag_of_words, y ,test_size=0.20,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxmqMjcsaRWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHEEelFPaRWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=transformer_bag_of_words.shape[1], init='normal', activation='relu'))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(512, init='normal', activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512, init='normal', activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512, init='normal', activation='relu'))\n",
        "    model.add(Dense(256, init='normal', activation='relu'))\n",
        "    model.add(Dense(128, init='normal', activation='relu'))\n",
        "    model.add(Dense(3, init='normal', activation=\"softmax\"))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=50, batch_size= 32, verbose= 1)\n",
        "# estimator.fit( X_train.todense(), y_train, validation_split=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cQDoEO0aRWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_i--1MaaRWR",
        "colab_type": "code",
        "outputId": "cff2de4b-dffa-430a-ba36-94690f93e775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "estimator.fit( X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 17:20:11.707036 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0728 17:20:11.760417 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 17:20:11.771626 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0728 17:20:11.799522 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0728 17:20:11.813558 140127358998400 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0728 17:20:11.950464 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 17:20:11.986961 140127358998400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0728 17:20:12.146305 140127358998400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "4223/4223 [==============================] - 43s 10ms/step - loss: 0.7823 - categorical_crossentropy: 0.7823\n",
            "Epoch 2/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.4612 - categorical_crossentropy: 0.4612\n",
            "Epoch 3/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.2445 - categorical_crossentropy: 0.2445\n",
            "Epoch 4/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.1859 - categorical_crossentropy: 0.1859\n",
            "Epoch 5/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0650 - categorical_crossentropy: 0.0650\n",
            "Epoch 6/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0249 - categorical_crossentropy: 0.0249\n",
            "Epoch 7/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0255 - categorical_crossentropy: 0.0255\n",
            "Epoch 8/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0135 - categorical_crossentropy: 0.0135\n",
            "Epoch 9/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0087 - categorical_crossentropy: 0.0087\n",
            "Epoch 10/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0122 - categorical_crossentropy: 0.0122\n",
            "Epoch 11/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0189 - categorical_crossentropy: 0.0189\n",
            "Epoch 12/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0247 - categorical_crossentropy: 0.0247\n",
            "Epoch 13/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0201 - categorical_crossentropy: 0.0201\n",
            "Epoch 14/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0262 - categorical_crossentropy: 0.0262\n",
            "Epoch 15/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0222 - categorical_crossentropy: 0.0222\n",
            "Epoch 16/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0110 - categorical_crossentropy: 0.0110\n",
            "Epoch 17/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0157 - categorical_crossentropy: 0.0157\n",
            "Epoch 18/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0247 - categorical_crossentropy: 0.0247\n",
            "Epoch 19/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0136 - categorical_crossentropy: 0.0136\n",
            "Epoch 20/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0202 - categorical_crossentropy: 0.0202\n",
            "Epoch 21/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0129 - categorical_crossentropy: 0.0129\n",
            "Epoch 22/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0147 - categorical_crossentropy: 0.0147\n",
            "Epoch 23/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0045 - categorical_crossentropy: 0.0045\n",
            "Epoch 24/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0130 - categorical_crossentropy: 0.0130\n",
            "Epoch 25/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0095 - categorical_crossentropy: 0.0095\n",
            "Epoch 26/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0295 - categorical_crossentropy: 0.0295\n",
            "Epoch 27/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0124 - categorical_crossentropy: 0.0124\n",
            "Epoch 28/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0073 - categorical_crossentropy: 0.0073\n",
            "Epoch 29/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0113 - categorical_crossentropy: 0.0113\n",
            "Epoch 30/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0114 - categorical_crossentropy: 0.0114\n",
            "Epoch 31/50\n",
            "4223/4223 [==============================] - 40s 10ms/step - loss: 0.0094 - categorical_crossentropy: 0.0094\n",
            "Epoch 32/50\n",
            "4223/4223 [==============================] - 41s 10ms/step - loss: 0.0146 - categorical_crossentropy: 0.0146\n",
            "Epoch 33/50\n",
            "4223/4223 [==============================] - 43s 10ms/step - loss: 0.0151 - categorical_crossentropy: 0.0151\n",
            "Epoch 34/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0131 - categorical_crossentropy: 0.0131\n",
            "Epoch 35/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0118 - categorical_crossentropy: 0.0118\n",
            "Epoch 36/50\n",
            "4223/4223 [==============================] - 43s 10ms/step - loss: 0.0261 - categorical_crossentropy: 0.0261\n",
            "Epoch 37/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0223 - categorical_crossentropy: 0.0223\n",
            "Epoch 38/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0222 - categorical_crossentropy: 0.0222\n",
            "Epoch 39/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0215 - categorical_crossentropy: 0.0215\n",
            "Epoch 40/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0202 - categorical_crossentropy: 0.0202\n",
            "Epoch 41/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0201 - categorical_crossentropy: 0.0201\n",
            "Epoch 42/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0256 - categorical_crossentropy: 0.0256\n",
            "Epoch 43/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0248 - categorical_crossentropy: 0.0248\n",
            "Epoch 44/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0179 - categorical_crossentropy: 0.0179\n",
            "Epoch 45/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0167 - categorical_crossentropy: 0.0167\n",
            "Epoch 46/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0168 - categorical_crossentropy: 0.0168\n",
            "Epoch 47/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0167 - categorical_crossentropy: 0.0167\n",
            "Epoch 48/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0166 - categorical_crossentropy: 0.0166\n",
            "Epoch 49/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0166 - categorical_crossentropy: 0.0166\n",
            "Epoch 50/50\n",
            "4223/4223 [==============================] - 42s 10ms/step - loss: 0.0164 - categorical_crossentropy: 0.0164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f719ebb3240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjN6kGsNaRWV",
        "colab_type": "code",
        "outputId": "d0846e4e-c96d-486f-a9d1-fa0285b368f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred= estimator.predict(X_test)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1056/1056 [==============================] - 2s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sVxwyXfaRWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXQ4iWVXaRWe",
        "colab_type": "code",
        "outputId": "d2884639-465f-4eac-e1cb-13df58f39575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, y_pred, average='macro')  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4543653616054589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smHIUqnWaRWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_transformer.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFWXyg2VaRWn",
        "colab_type": "code",
        "outputId": "4a38b605-7f64-4f7e-b230-4b7af90188dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_submission= estimator.predict(X_test_transformer.todense())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2924/2924 [==============================] - 4s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkJw15hGaRWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_tfidf_RNN = pd.DataFrame({'unique_hash':test_df[\"unique_hash\"],'sentiment':y_submission})\n",
        "\n",
        "# #Visualize the first 5 rows\n",
        "# submission3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTi7p1sHaRWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_tfidf_RNN.to_csv(\"Innoplexus_finalday_submission_8.csv\", index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQYod98QsG8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Innoplexus_finalday_submission_8.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p3PTfv0aRW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # total numberof extracted words.\n",
        "# vocab=w2v_model.wv.vocab\n",
        "# print(\"The total number of words are : \",len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yM8gH9_aRW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# w2v_weights = w2v_model.wv.vectors\n",
        "# w2v_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-MQpWzJaRW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_vec_dict={}\n",
        "# for word in vocab:\n",
        "#     word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
        "#     print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCoJDjGyaRW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # cleaning reviews.\n",
        "# train_df['cleaned_text']=train_df['text'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQTmzTIfaRW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(train_df[\"cleaned_text\"][100].split())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSEZ-2jgaRXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # number of unique words = \n",
        "\n",
        "# # now since we will have to pad we need to find the maximum lenght of any document.\n",
        "\n",
        "# maxim=-1\n",
        "# for i,rev in enumerate(train_df['cleaned_text']):\n",
        "# #   tokens=rev.split()\n",
        "#     if(len(rev.split()) > maxim):\n",
        "#         maxim=len(rev.split())\n",
        "#         print(maxim)\n",
        "#         print(i)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsP1jjghaRXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  df_new= train_df.drop(train_df.index[[15, 46, 1374, 1457, 1657]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyd9oy16aRXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning reviews.\n",
        "# df_new['cleaned_text']=df_new['text'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxfjdpPjaRXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # now since we will have to pad we need to find the maximum lenght of any document.\n",
        "\n",
        "# maxi=-1\n",
        "# for i,rev in enumerate(train_df['cleaned_text']):\n",
        "#     tokens=rev.split()\n",
        "#     if(len(tokens)>maxi):\n",
        "#         maxi=len(rev.split())\n",
        "        \n",
        "#         print(i)\n",
        "# print(maxi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doAjo09jaRXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_df[\"cleaned_text\"][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY--6_0UaRXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhrN2bY3aRXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# tok = Tokenizer()\n",
        "# tok.fit_on_texts(train_df['cleaned_text'])\n",
        "# vocab_size = len(tok.word_index) + 1\n",
        "# encd_rev = tok.texts_to_sequences(train_df['cleaned_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q-JnoAbaRXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_seq_len=maxim +1 # max lenght of a review\n",
        "# vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "# embedding_size=100 # embedding dimension as choosen in word2vec constructor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHievERoaRXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh4yjQMIaRXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # pad sequence to max_len\n",
        "# pad_rev= pad_sequences(encd_rev, maxlen=max_seq_len, padding='post')\n",
        "# pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_vDfzduaRXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # now creating the embedding matrix\n",
        "# embed_matrix=np.zeros(shape=(vocab_size,embedding_size))\n",
        "# for word,i in tok.word_index.items():\n",
        "#     embed_vector=word_vec_dict.get(word)\n",
        "#     if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
        "#         embed_matrix[i]=embed_vector\n",
        "#         # if word is not found then embed_vector corressponding to that vector will stay zero."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0lBeMccaRXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tfidf_vector_Y = Y[:, :, None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_lAvEyoaRX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # prepare train and validation sets \n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # Y = pd.get_dummies(train_df['sentiment']).values  # one hot target as required by NN.\n",
        "# X_train,X_test,y_train,y_test=train_test_split(array_X_train, Y,test_size=0.20,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOHI4_eraRX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc_Gh0ImaRX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "# #X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oq1oRHZaRX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xvhSzCaRYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(LSTM(units=24, input_dim = array_X_train.shape[1], return_sequences = True))\n",
        "# model.add(LSTM(units=12, return_sequences=True))\n",
        "# model.add(LSTM(units=6, return_sequences=True))\n",
        "# model.add(LSTM(units=3, return_sequences=True, name='output'))\n",
        "# model.compile(loss='cosine_proximity', optimizer='sgd', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paGLn4P6aRYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose= 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqubS2cGaRYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_out = 64\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim= vocab_size, output_dim=embedding_size ,weights= [embed_matrix], input_length = max_seq_len))\n",
        "# model.add(SpatialDropout1D(0.4))\n",
        "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(3,activation='softmax'))\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lqJwlgDaRYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                  np.unique(y_train),\n",
        "#                                                  y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQpTjbndaRYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history = model.fit(x_train, y_train, epochs=5, batch_size=32, verbose= 1)\n",
        "# #                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qWiB1HxaRYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plotting Loss and Accuracy Graphs\n",
        "# plt.figure(figsize=(12, 12))\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Loss')\n",
        "# plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(12, 12))\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('Accuracy')\n",
        "# plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itHQoFtGaRYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to disk\n",
        "# filename = 'finalized_model.sav'\n",
        "# pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Npp4F2paRYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Let's check for NULL values\n",
        "# test_df.isnull().values.any()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84sjdu_GaRYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
        "# from keras.models import Sequential\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeuTeY1zaRYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_features = 8000\n",
        "# max_len = 150\n",
        "# tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# tokenizer.fit_on_texts(train_df['text'].values)\n",
        "# X = tokenizer.texts_to_sequences(train_df['text'].values)\n",
        "# X = pad_sequences(X, maxlen= max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npQIlPgVaRYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLWAcEqEaRYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embed_dim = 128\n",
        "# lstm_out = 64\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim= vocab_size, output_dim=embedding_size ,weights= , input_length = X.shape[1]))\n",
        "# model.add(SpatialDropout1D(0.4))\n",
        "# model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\n",
        "# model.add(Dense(3,activation='softmax'))\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lkW0urbaRYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y = pd.get_dummies(train_df['sentiment']).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymSMa9UyaRYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                  np.unique(y_train),\n",
        "#                                                  y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRBwAAXjaRYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "# print(X_train.shape,Y_train.shape)\n",
        "# print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ozyfd_aRYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size = 32\n",
        "# model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuOHYYgcaRYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accr = model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-wheTM9aRYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_pred= model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-biJCM6aRYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trxMnWpLaRYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_label=[]\n",
        "# for i in range(len(X_test)):\n",
        "#     Y_label.append(np.argmax(Y_pred[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxXiAZMzaRYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgFsYQE4aRYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_labels= train_df[\"sentiment\"].values\n",
        "# _, _, _, Y_test_labels = train_test_split(X,Y_labels, test_size = 0.33, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPTgxgSYaRY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlTasWdaRY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f1_score(Y_test_labels, Y_label, average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Mw0br6aRY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_features = 8000\n",
        "# max_len = 150\n",
        "# tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# tokenizer.fit_on_texts(test_df['text'].values)\n",
        "# X_submission = tokenizer.texts_to_sequences(test_df['text'].values)\n",
        "# X_submission = pad_sequences(X, maxlen= max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74rQ7lSdaRY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_submission_prob= model.predict(X_submission)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmPWw29JaRY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_submission_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKNHXJjvaRY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y_label_submissions=[]\n",
        "# for i in range(len(test_df)):\n",
        "#     Y_label_submissions.append(np.argmax(Y_submission_prob[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFgglEVNaRY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission5 = pd.DataFrame({'unique_hash':test_df[\"unique_hash\"],'sentiment':Y_label_submissions})\n",
        "\n",
        "# #Visualize the first 5 rows\n",
        "# submission4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utnZ8WbwaRZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission5.to_csv(\"Innoplexus5.csv\", index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hTVZYXaaRZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to disk\n",
        "# filename = 'finalized_model.sav'\n",
        "# pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlTvjlkaRZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load the model from disk\n",
        "# loaded_model = pickle.load(open(filename, 'rb'))\n",
        "# result = loaded_model.score(X_test, Y_test)\n",
        "# print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1K90skZaRZJ",
        "colab_type": "text"
      },
      "source": [
        "**Luckily, Our dataset has no missing values, a sigh of relief for us! No imputations needed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK8DOyTkaRZK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VomoEENdaRZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tv = TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb')\n",
        "# X= tv.fit_transform(df.processed).toarray()\n",
        "# X =pd.DataFrame(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSUPQ3KbaRZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import spacy\n",
        "\n",
        "# from gensim.corpora import Dictionary\n",
        "# from gensim.models.tfidfmodel import TfidfModel\n",
        "# from gensim.matutils import sparse2full"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__uxlfxkaRZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def weightedword2vec(df):\n",
        "#     review_para_list = df[\"processed\"].tolist()\n",
        "    \n",
        "#     # be sure to split sentence before feed into Dictionary\n",
        "#     dataset = [d.split() for d in review_para_list]\n",
        "    \n",
        "#     nlp  = spacy.load('en_core_web_sm')\n",
        "    \n",
        "#     data_dict = Dictionary(dataset)\n",
        "#     data_dict.filter_extremes()\n",
        "#     data_dict.compactify()\n",
        "    \n",
        "#     docs_corpus = [data_dict.doc2bow(data) for data in dataset]\n",
        "#     model_tfidf = TfidfModel(docs_corpus, id2word=data_dict)\n",
        "#     docs_tfidf  = model_tfidf[docs_corpus]\n",
        "#     docs_vecs   = np.vstack([sparse2full(c, len(data_dict)) for c in docs_tfidf])\n",
        "    \n",
        "#     tfidf_emb_vecs = np.vstack([nlp(data_dict[i]).vector for i in range(len(data_dict))])\n",
        "    \n",
        "#     docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) \n",
        "    \n",
        "#     return docs_emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS9Bj05aaRZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data_embedded= weightedword2vec(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVMLgidVaRZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_data_embedded= weightedword2vec(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIEc8EGdaRZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# train_pca = PCA(n_components=15).fit_transform(train_data_embedded)\n",
        "# test_pca = PCA(n_components=15).fit_transform(test_data_embedded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNXVbS-aRZi",
        "colab_type": "text"
      },
      "source": [
        "In spaCy, you can load a model via spacy.load(). This will return a Language object containing all components and data needed to process text. We usually call it nlp. Calling the nlp object on a string of text will return a processed Doc. The advantage of this  method is that the entire info of the text is still preserved. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek_Vc_6aaRZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X= train_pca\n",
        "# y= train_df[\"sentiment\"].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIn7rjb5aRZo",
        "colab_type": "text"
      },
      "source": [
        "The shape of our dataset is (5279, 10840), which means the no of data points are 5279 and we have generated 10840 different features, or we have 10840 different words \n",
        "\n",
        "Now, we'll calculate word2vec values (300 dimensions) for each of these unique tf-idf terms, and then multiply the same with the tfidf matrix to get a **tf-idf weighted word2ve**c matrix which we'll use in our modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-i504GIaRZo",
        "colab_type": "text"
      },
      "source": [
        "* Our tf-idf matrix (doc_vecs) has a dimension of 5279 * 3000 \n",
        "* The tfid_emb_vec has a dimension of 3000 * 300\n",
        "\n",
        "**The resultant TF-IDF weighted matrix has a dimension of 5279 x 300 (no. of data points * embedding dimension)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrZVrujjaRZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# ros= RandomOverSampler()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW81V2WUaRZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_resampled, y_resampled= ros.fit_sample(train_pca, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJf03QvLaRZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "#   # 80 % go into the training test, 20% in the validation test\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.3, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1aY1I1KaRZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_resampled, y_resampled= ros.fit_sample(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MIt3YNkaRZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxSQStYSaRZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rcf= RandomForestClassifier()\n",
        "# rcf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDTAvvrDaRZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_pred= rcf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brljuJmIaRZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.unique(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5SsOd-MaRZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f1_score(y_test, y_pred, average='macro')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSrqLEhPaRZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_submission =rcf.predict(test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0D-EfKwaRZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.unique(y_submission)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ3UFh0paRZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission3 = pd.DataFrame({'unique_hash':test_df[\"unique_hash\"],'sentiment':y_submission})\n",
        "\n",
        "# #Visualize the first 5 rows\n",
        "# submission3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-fgSC0yaRZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission3.to_csv(\"Innoplexus3.csv\", index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}